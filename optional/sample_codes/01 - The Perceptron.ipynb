{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by importing some of the libraries we will need and set up our notebook session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Basic Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here goes the blueprint of our perceptron, encoded as a plain Python class, as well as a single auxiliary function. Below we will walk through the details of each code components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, nb_features=None):\n",
    "        self.weights = None\n",
    "        self.nb_features = nb_features # i.e. the nb of incoming connections from other neurons\n",
    "    \n",
    "    def set_weights(self, weights=None):\n",
    "        if weights:\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = np.ones(self.nb_features)\n",
    "    \n",
    "    def predict(self, feature_vectors, squash=False):\n",
    "        scores = []\n",
    "        for fv in feature_vectors:\n",
    "            s = 0.0\n",
    "            for i in range(self.nb_features):\n",
    "                s += self.weights[i] * fv[i]\n",
    "            scores.append(s)\n",
    "        \n",
    "        if squash:\n",
    "            scores = [sigmoid(v) for v in scores]\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by predicting house prices in a really naive way, i.e. we assume that all characteristics are equally important, each having a (positive) weight of 1. We characterize each house along 5 integer variables, namely:\n",
    "* the number of doors\n",
    "* surface (in square meters)\n",
    "* the number of bedrooms\n",
    "* the number of bathrooms\n",
    "* its age (in years)\n",
    "\n",
    "First, we initialize our perceptron, and we specify that we will use 5 input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron(nb_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the weights of our perceptron; by default, our perceptron will assign an equal, positive weight of 1.0 to each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perceptron.set_weights()\n",
    "print(perceptron.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a dummy data set of 5 houses, which are represented as a fixed length vector of five integers (the index of which corresponds to the bullet list above): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "houses = [[2, 3, 5, 1, 8],\n",
    "          [1, 2, 1, 3, 5],\n",
    "          [4, 4, 2, 1, 3],\n",
    "          [11, 6, 8, 9, 8],\n",
    "          [9, 10, 8, 1, 30]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data set, the last houses have higher a number of doors, bathrooms etc., so that they would have to predict higher prices. We now feed this data set to the `predict()` function of our perceptron (cf. standard `sklearn` naming conventions), and we have it predict a the price for each house:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prices = perceptron.predict(houses)\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the see that for instance the pentultimate house has a relatively higher price. Of course, our feature weighting is now ridiculously naive. Clearly,\n",
    "* the number of bathrooms should matter relative more than the number of doors\n",
    "* the age of a house should negatively affect the total price\n",
    "\n",
    "We can now fix this by setting a more sensible weight vector, which should have five entries (one for each feature). Using these new weights, we should now obtain a more accurate estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = [0.5, # doors\n",
    "           0.9, # surface\n",
    "           0.8, # bedrooms\n",
    "           0.9, # bathrooms\n",
    "           -0.5] # age\n",
    "perceptron.set_weights(weights)\n",
    "print(perceptron.predict(houses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the price of a house is simple enough and comes down to calculating the **weighted sum** of the feature values. We now see, for instance, that the final house will be valued much less than the penultimate house in the list, based on the assumption that, while is it much larger, it is also is much older and will therefore require much more investments. Note that the prices predicted are not in an actual currency and vary wildly. In many applications, it is very common to **normalize** the predictions of a perceptron and 'squash' them to a range between 0 and 1. Historically, the **sigmoid** function has often been for this. By setting `squash = True`, we can implement this behaviour in our perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(perceptron.predict(houses, squash=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that we obtain predictions which are neatly between 0 and 1. In the context of deep learning, functions such as the sigmoid function are often called **activation** functions, because they control how strongly a particular neuron will be activated. In our perceptron, we have a single output neuron, the activation of which is controlled via a sigmoid activation. We call such an activation **element-wise**, is they are applied to each element in a list individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up our perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron which we created will be slow, because, right now, it only relies on traditional, unoptimized Python code. Inspect for instance the code block in the `predict()` function above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The many `for`-loops in this code will make it extremely slow to run. In Deep Learning (and in so many other fields), such code will be too slow to be usable in practice: it is much more common to used **vectorized routines** from specialized libraries, such as `numpy`, which can easily replace our naive `for`-loops. To be able to vectorize our code more efficiently, we will have to convert our Python lists to `numpy` arrays, which essentially are matrix objects, that have a **`shape`** attribute. As will become clear below, the `shape` is an extremely important property of `numpy` arrays, and you will want to keep track of it frequently, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "houses = np.array(houses)\n",
    "print(houses.shape)\n",
    "\n",
    "weights = np.array(weights)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that our dummy data set is now represented by a two-dimensional matrix which has 5 rows (corresponding to the number of houses) and 5 columns (corresponding to the number of features). Our `weights` variable, on the other hand is a single-dimensional vector, instead of a matrix, of consisting of five numbers (scalars). We are now ready to redefine our Perceptron blueprint to be able to deal with such arrays more efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def v_sigmoid(x):\n",
    "    # element-wise; eats any tensor\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class VectorizedPerceptron:\n",
    "    def __init__(self, nb_features=None):\n",
    "        self.weights = None\n",
    "        self.nb_features = nb_features\n",
    "    \n",
    "    def set_weights(self, weights=None):\n",
    "        if isinstance(weights, np.ndarray):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = np.ones(self.nb_features)\n",
    "    \n",
    "    def predict(self, feature_vectors, squash=False):\n",
    "        scores = np.multiply(feature_vectors, self.weights).sum(axis=-1)\n",
    "        \n",
    "        if squash:\n",
    "            scores = v_sigmoid(scores)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, once you get used to such vectorized notation, vectorized code becomes much easier to read than stacks of for-loops. The result is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v_perceptron = VectorizedPerceptron()\n",
    "v_perceptron.set_weights(weights)\n",
    "\n",
    "print(perceptron.predict(houses))\n",
    "print(perceptron.predict(houses, squash=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But is this faster? Let us create an artificial, large house data set to check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "houses = np.random.uniform(low=-0.05, high=0.05, size=(1000, 500))\n",
    "print(houses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simple:\n",
    "weights = list(np.random.uniform(low=-0.05, high=0.05, size=(500)))\n",
    "perceptron = Perceptron(nb_features=500)\n",
    "perceptron.set_weights(weights)\n",
    "\n",
    "# vectorized:\n",
    "v_perceptron = VectorizedPerceptron(nb_features=500)\n",
    "v_perceptron.set_weights(np.array(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can time the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit perceptron.predict(houses, squash=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit v_perceptron.predict(houses, squash=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jip, it seems to be quite a bit faster -- and believe me, the difference grows even larger for larger data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first implement a pre-historic approach to optimizing our perceptron. We first load the Boston Housing Prices Dataset that ships with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this data set has information for 506 houses across 13 numerical features. The target vector which we like to fit is a list of 506 prices associated with each house:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start again with a stripped down version of our `Perceptron` class: upon instantiation, objects from this class will create a list of weight parameters, or really small values randomly sampled from a uniform distribution betwee: -0.05 and +0.05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, nb_features=None):\n",
    "        np.random.seed(156651)\n",
    "        self.weights = np.random.uniform(low=-.05, high=0.05,\n",
    "                                         size=nb_features)\n",
    "    \n",
    "    def predict(self, feature_vectors):\n",
    "        return np.multiply(feature_vectors, self.weights).sum(axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, our `Perceptron` has a `predict` method, which we can use to obtain house prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron()\n",
    "prices = perceptron.predict(X)\n",
    "print(prices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to measure the current quality of our system, we need a **loss function** or **objective function**: later, we will try to get down the loss returned by the function. As a loss function, we start off with the relatively **mean squared error**: it compares the proces outputted by the system and the **gold standard** (in the `y` vector) and returns the mean of squares with respect to the difference between each predicted price and its gold standard equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_gold, y_pred):\n",
    "    return np.mean((y_gold - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find out how large the loss is for our current model, which has randomly initialized parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = perceptron.predict(X)\n",
    "print(mean_squared_error(preds, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's huge! We are now ready to add a naive `fit` method to optimize the parameters of our perceptron. In each **epoch** (i.e. one pass over the entire data set), we loop over each parameter individually and test two different situations: *hypothesis a*, in which we slightly increase it (with a small rate `learning_rate`) and *hypothesis b*, in which we slightly decrease the weight under scrutiny (again with a small `learning` rate). Then, we adjust the parameters in the light of whichever hypothesis gave the best result (i.e. maximally minimized the loss). That goes like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, nb_features=None):\n",
    "        self.nb_features = nb_features\n",
    "        np.random.seed(156651)\n",
    "        self.weights = np.array(np.random.uniform(low=-.05, high=0.5,\n",
    "                                         size=self.nb_features))\n",
    "    \n",
    "    def predict(self, feature_vectors, weights=None):\n",
    "        if not isinstance(weights, np.ndarray):\n",
    "            return np.multiply(feature_vectors, self.weights).sum(axis=-1)\n",
    "        else:\n",
    "            return np.multiply(feature_vectors, weights).sum(axis=-1)\n",
    "    \n",
    "    def fit(self, X, y, learning_rate=0.1, nb_epochs=10):\n",
    "        losses = []\n",
    "        \n",
    "        for e in range(nb_epochs):\n",
    "            if e % 100 == 0 and e:\n",
    "                learning_rate *= 0.9\n",
    "            \n",
    "            for idx in range(self.nb_features):\n",
    "                # hypothesis a: we increase the parameter:\n",
    "                weights_plus = self.weights.copy()\n",
    "                weights_plus[idx] += learning_rate\n",
    "\n",
    "                # hypothesis b: we decrease the parameter:\n",
    "                weights_minus = self.weights.copy()\n",
    "                weights_minus[idx] -= learning_rate\n",
    "                \n",
    "                # we obtain the predictions for both hypotheses a and b:\n",
    "                plus_preds = self.predict(X, weights = weights_plus)\n",
    "                minus_preds = self.predict(X, weights = weights_minus)\n",
    "                \n",
    "                # we obtain the predictions for both hypotheses a and b:\n",
    "                plus_loss = mean_squared_error(plus_preds, y)\n",
    "                minus_loss = mean_squared_error(minus_preds, y)\n",
    "                \n",
    "                # we adjust \n",
    "                if plus_loss < minus_loss:\n",
    "                    self.weights = weights_plus.copy()\n",
    "                elif minus_loss < plus_loss:\n",
    "                    self.weights = weights_minus.copy()\n",
    "            \n",
    "            # we calculate the loss\n",
    "            loss = mean_squared_error(self.predict(X), y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if e and e % 20 == 0:\n",
    "                print('Loss after epoch #', e + 1, '->', loss)\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now apply this fit method with a small enough learning rate, you see that we are able to gradually push down the loss in the subsequent epochs, until the loss curve staurates and stabilizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = Perceptron(nb_features=X.shape[1])\n",
    "p.predict(X)\n",
    "losses = p.fit(X, y, nb_epochs=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graphical representation of the loss curve is easy enough to obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do It Yourself:\n",
    "\n",
    "Add some simple modifications to the code above. Note that these exercises in fact introduce some key concepts that are very central in modern deep learning research.\n",
    "\n",
    "* Adapt our Perceptron to show an adaptive, **decreasing learning rate**: after a number of epochs (e.g. 100), it would make sense to decrease the learning steps taken, for instance by a factor of three.\n",
    "* Implement a form of **early stopping**: stop the training procedure, if the loss doesn't significantly go down anymore for a fixed number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Perceptron in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For the sake of reference, I include a similar implementation of our perceptron in `Tensorflow`, but we probably won't have tim eto work through block by block...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X = np.array(X, dtype='float32')\n",
    "y = np.array(y, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# purely \"symbolic variable\"\n",
    "train_input = tf.placeholder(tf.float32, [None, X.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_output = tf.placeholder(tf.float32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng.seed(156651)\n",
    "weights = tf.Variable(np.asarray(rng.uniform(low=-0.05, high=0.05,\n",
    "                                 size=(X.shape[1])),\n",
    "                     dtype='float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_output = tf.reduce_sum(tf.mul(train_input, weights), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse_cost = tf.reduce_mean(tf.square(model_output - train_output),\n",
    "                          reduction_indices=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.0000001).minimize(mse_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = sess.run(model_output, feed_dict={train_input: X})\n",
    "print(mean_squared_error(f, y))\n",
    "g = sess.run(mse_cost, feed_dict={train_input: X, train_output: y})\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(500):\n",
    "    c = sess.run(mse_cost, feed_dict={train_input: X, train_output: y})\n",
    "    losses.append(c)\n",
    "    if i % 50 == 0:\n",
    "        print(c)\n",
    "    sess.run(train_step, feed_dict={train_input: X, train_output: y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addendum\n",
    "\n",
    "There is an additional notebook in the repo, i.e. [A simple implementation of ANN for MNIST](1.4 (Extra) A Simple Implementation of ANN for MNIST.ipynb) for a *naive* implementation of **SGD** and **MLP** applied on **MNIST** dataset.\n",
    "\n",
    "This accompanies the online text http://neuralnetworksanddeeplearning.com/ . The book is highly recommended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
